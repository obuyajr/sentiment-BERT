# sentiment-BERT
Sentiment Analysis using BERT Model
Introduction
This repository contains code for performing sentiment analysis using BERT model. BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google that can be fine-tuned for various NLP tasks such as sentiment analysis, text classification, and question answering.

The code in this repository is written in Python and uses the PyTorch framework. It uses the Hugging Face Transformers library, which provides an easy-to-use interface for working with BERT and other pre-trained transformer models.
